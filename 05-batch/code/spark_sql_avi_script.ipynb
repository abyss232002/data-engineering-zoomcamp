{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'dezoomcamp2024_project'\n",
    "spark_sql = (\n",
    "    SparkSession.builder.master(\"spark://dezoomcamp2024.us-east1-b.c.dezoomcamp2024-week5.internal:7077\").appName(\"App_local_cluster\")\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.15.1-beta,com.google.cloud.bigdataoss:gcs-connector:hadoop2-2.1.6\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark_sql._jsc.hadoopConfiguration().set(\n",
    "    \"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\"\n",
    ")\n",
    "spark_sql.conf.set(\"temporaryGcsBucket\", bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = spark_sql.read.parquet(\"gs://dezoomcamp2024_project/processed/green/2020/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark_sql.read.parquet(\"gs://dezoomcamp2024_project/processed/yellow/2020/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow \\\n",
    "            .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime') \\\n",
    "            .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df_green \\\n",
    "            .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime') \\\n",
    "            .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a common column list which is present in both green and yellow dataframes and order them same as green dataframe\n",
    "common_columns = []\n",
    "yellow_columns = df_yellow.columns\n",
    "for column in df_green.columns:\n",
    "    if column in yellow_columns:\n",
    "        common_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add service type column to green dataframe and yellow dataframe\n",
    "from pyspark.sql import functions as F\n",
    "df_green = df_green.withColumn('service_type', F.lit('green'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow.withColumn('service_type', F.lit('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_select = df_yellow.select(common_columns + ['service_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_select = df_green.select(common_columns + ['service_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union both green and yellow dataframes\n",
    "df_trip_data = df_green_select.unionAll(df_yellow_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary table from the dataframe\n",
    "df_trip_data.registerTempTable('trip_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark_sql.sql('''\n",
    "select \n",
    "    -- Reveneue grouping \n",
    "    PULocationID as revenue_zone,\n",
    "    date_trunc(\"month\", \"pickup_datetime\") as revenue_month, \n",
    "    service_type, \n",
    "\n",
    "    -- Revenue calculation \n",
    "    sum(fare_amount) as revenue_monthly_fare,\n",
    "    sum(extra) as revenue_monthly_extra,\n",
    "    sum(mta_tax) as revenue_monthly_mta_tax,\n",
    "    sum(tip_amount) as revenue_monthly_tip_amount,\n",
    "    sum(tolls_amount) as revenue_monthly_tolls_amount,\n",
    "    sum(improvement_surcharge) as revenue_monthly_improvement_surcharge,\n",
    "    sum(total_amount) as revenue_monthly_total_amount,\n",
    "\n",
    "    -- Additional calculations\n",
    "    avg(passenger_count) as avg_monthly_passenger_count,\n",
    "    avg(trip_distance) as avg_monthly_trip_distance\n",
    "\n",
    "from trip_data\n",
    "group by 1,2,3\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.repartition(4).write.parquet('data/report/revenue', mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
